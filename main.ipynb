{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load, Split, and Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# drop rows with missing values\n",
    "data = data.dropna()\n",
    "\n",
    "#encode the string data as integers\n",
    "label_encoder = LabelEncoder()\n",
    "data['State'] = label_encoder.fit_transform(data['State'])\n",
    "data['County'] = label_encoder.fit_transform(data['County'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we've decided to keep the county variable because by keeping the county variable, we could find patterns or trends specific to that county that would otherwise be lost if we removed the variable. However, since there are so many counties, it could make the model more complex and this could lead to overfitting. If it ends up causing overfitting, we'll consider removing it later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balancing Dataset\n",
    "\n",
    "We chose to use quantization thresholds for the ChildPoverty variable and divided them into 4 classes. By doing this, we ensure that each class has an approximately equal number of instances, which helps in balancing the dataset. Also, by using quantization thresholds we deal with the continuous variables by converting them into categorical variables, making them suitable for classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should only balance the training dataset because this ensures that model learns equally from all classes, preventing bias towards any particular class. We shouldn't balance the test dataset because it should  represent the real-world distribution of the data. Balancing the testing set would artificially alter the distribution of classes, leading to a bias in the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data['ChildPovertyClass'] = pd.qcut(data['ChildPoverty'], 4, labels=False)\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42, stratify=data['ChildPovertyClass'])\n",
    "\n",
    "# Balance the training set\n",
    "train_data = train_data.groupby('ChildPovertyClass').apply(lambda x: x.sample(train_data['ChildPovertyClass'].value_counts().min())).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100/100"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (14544, 1), indices imply (4, 14544)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 182\u001b[0m\n\u001b[0;32m    179\u001b[0m nn \u001b[38;5;241m=\u001b[39m TwoLayerPerceptronVectorized(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m    181\u001b[0m nn\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, print_progress\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[1;32m--> 182\u001b[0m yhat \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m    183\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(y_test, yhat)\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[17], line 83\u001b[0m, in \u001b[0;36mTwoLayerPerceptron.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Predict class labels\"\"\"\u001b[39;00m\n\u001b[0;32m     82\u001b[0m _, _, _, _, A3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feedforward(X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW1, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW2, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb1, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb2)\n\u001b[1;32m---> 83\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(A3, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y_pred\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36margmax\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\mmthe_22\\anaconda3\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:1242\u001b[0m, in \u001b[0;36margmax\u001b[1;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[0;32m   1155\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1156\u001b[0m \u001b[38;5;124;03mReturns the indices of the maximum values along an axis.\u001b[39;00m\n\u001b[0;32m   1157\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1239\u001b[0m \u001b[38;5;124;03m(2, 1, 4)\u001b[39;00m\n\u001b[0;32m   1240\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1241\u001b[0m kwds \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeepdims\u001b[39m\u001b[38;5;124m'\u001b[39m: keepdims} \u001b[38;5;28;01mif\u001b[39;00m keepdims \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39m_NoValue \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m-> 1242\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _wrapfunc(a, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124margmax\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[1;32mc:\\Users\\mmthe_22\\anaconda3\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:54\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     52\u001b[0m bound \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, method, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bound \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bound(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[1;32mc:\\Users\\mmthe_22\\anaconda3\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:47\u001b[0m, in \u001b[0;36m_wrapit\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, mu\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m     46\u001b[0m         result \u001b[38;5;241m=\u001b[39m asarray(result)\n\u001b[1;32m---> 47\u001b[0m     result \u001b[38;5;241m=\u001b[39m wrap(result)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\mmthe_22\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:2107\u001b[0m, in \u001b[0;36mNDFrame.__array_wrap__\u001b[1;34m(self, result, context)\u001b[0m\n\u001b[0;32m   2105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[0;32m   2106\u001b[0m d \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_axes_dict(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_AXIS_ORDERS, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m-> 2107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor(res, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39md)\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__array_wrap__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\mmthe_22\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:722\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    712\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m dict_to_mgr(\n\u001b[0;32m    713\u001b[0m             \u001b[38;5;66;03m# error: Item \"ndarray\" of \"Union[ndarray, Series, Index]\" has no\u001b[39;00m\n\u001b[0;32m    714\u001b[0m             \u001b[38;5;66;03m# attribute \"name\"\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    719\u001b[0m             typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[0;32m    720\u001b[0m         )\n\u001b[0;32m    721\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 722\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m ndarray_to_mgr(\n\u001b[0;32m    723\u001b[0m             data,\n\u001b[0;32m    724\u001b[0m             index,\n\u001b[0;32m    725\u001b[0m             columns,\n\u001b[0;32m    726\u001b[0m             dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m    727\u001b[0m             copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m    728\u001b[0m             typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[0;32m    729\u001b[0m         )\n\u001b[0;32m    731\u001b[0m \u001b[38;5;66;03m# For data is list-like, or Iterable (will consume into list)\u001b[39;00m\n\u001b[0;32m    732\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(data):\n",
      "File \u001b[1;32mc:\\Users\\mmthe_22\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:349\u001b[0m, in \u001b[0;36mndarray_to_mgr\u001b[1;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# _prep_ndarraylike ensures that values.ndim == 2 at this point\u001b[39;00m\n\u001b[0;32m    345\u001b[0m index, columns \u001b[38;5;241m=\u001b[39m _get_axes(\n\u001b[0;32m    346\u001b[0m     values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], index\u001b[38;5;241m=\u001b[39mindex, columns\u001b[38;5;241m=\u001b[39mcolumns\n\u001b[0;32m    347\u001b[0m )\n\u001b[1;32m--> 349\u001b[0m _check_values_indices_shape_match(values, index, columns)\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    353\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\mmthe_22\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:420\u001b[0m, in \u001b[0;36m_check_values_indices_shape_match\u001b[1;34m(values, index, columns)\u001b[0m\n\u001b[0;32m    418\u001b[0m passed \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    419\u001b[0m implied \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlen\u001b[39m(index), \u001b[38;5;28mlen\u001b[39m(columns))\n\u001b[1;32m--> 420\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of passed values is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpassed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, indices imply \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimplied\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Shape of passed values is (14544, 1), indices imply (4, 14544)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "import sys\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "## this code is taken directly from the Github\n",
    "class TwoLayerPerceptronBase(object):\n",
    "    def __init__(self, n_hidden=30,\n",
    "                 C=0.0, epochs=500, eta=0.001, random_state=None):\n",
    "        np.random.seed(random_state)\n",
    "        \n",
    "        self.n_hidden = n_hidden\n",
    "        self.l2_C = C\n",
    "        self.epochs = epochs\n",
    "        self.eta = eta\n",
    "        \n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights with Glorot.\"\"\"\n",
    "        limit1 = np.sqrt(6 / (self.n_features_ + self.n_hidden))\n",
    "        W1 = np.random.uniform(-limit1, limit1, (self.n_hidden, self.n_features_))\n",
    "        b1 = np.zeros((self.n_hidden, 1))\n",
    "        \n",
    "        limit2 = np.sqrt(6 / (self.n_hidden + self.n_output_))\n",
    "        W2 = np.random.uniform(-limit2, limit2, (self.n_output_, self.n_hidden))\n",
    "        b2 = np.zeros((self.n_output_, 1))\n",
    "        \n",
    "        return W1, W2, b1, b2\n",
    "    \n",
    "    @staticmethod\n",
    "    def _sigmoid(z):\n",
    "        \"\"\"Use scipy.special.expit to avoid overflow\"\"\"\n",
    "        # 1.0 / (1.0 + np.exp(-z))\n",
    "        return expit(z)\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def _L2_reg(lambda_, W1, W2):\n",
    "        \"\"\"Compute L2-regularization cost\"\"\"\n",
    "        # only compute for non-bias terms\n",
    "        return (lambda_) * np.sqrt(np.mean(W1 ** 2) + np.mean(W2 ** 2))\n",
    "    \n",
    "    def _cost(self,A3,Y,W1,W2):\n",
    "        '''Get the objective function value'''\n",
    "        m = y.shape[0]\n",
    "        y_expanded = np.zeros_like(A3)\n",
    "        y_expanded[y, np.arange(m)] = 1\n",
    "        \n",
    "        # Binary cross-entropy for each class prediction\n",
    "        log_probs = -y_expanded * np.log(A3) - (1 - y_expanded) * np.log(1 - A3)\n",
    "        cost = np.sum(log_probs) / m\n",
    "        L2_term = self._L2_reg(self.l2_C, W1, W2)\n",
    "        return cost + L2_term\n",
    "    \n",
    "# now let's add in the following functions:\n",
    "#    feedforward\n",
    "#    fit and predict\n",
    "class TwoLayerPerceptron(TwoLayerPerceptronBase):\n",
    "    def _feedforward(self, X, W1, W2, b1, b2):\n",
    "        \"\"\"Compute feedforward step\n",
    "        -----------\n",
    "        X : Input layer with original features.\n",
    "        W1: Weight matrix for input layer -> hidden layer.\n",
    "        W2: Weight matrix for hidden layer -> output layer.\n",
    "        ----------\n",
    "        a1-a3 : activations into layer (or output layer)\n",
    "        z1-z2 : layer inputs \n",
    "\n",
    "        \"\"\"\n",
    "        A1 = X.T\n",
    "        Z1 = W1 @ A1 + b1\n",
    "        A2 = self._sigmoid(Z1)\n",
    "        Z2 = W2 @ A2 + b2\n",
    "        A3 = self._sigmoid(Z2)\n",
    "        return A1, Z1, A2, Z2, A3\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\"\"\"\n",
    "        _, _, _, _, A3 = self._feedforward(X, self.W1, self.W2, self.b1, self.b2)\n",
    "        y_pred = np.argmax(A3, axis=0) \n",
    "        return y_pred\n",
    "    \n",
    "    \n",
    " \n",
    "            \n",
    "class TwoLayerPerceptronVectorized(TwoLayerPerceptron):\n",
    "    def _get_gradient(self, A1, A2, A3, Z1, Z2, Y, W1, W2):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "        \"\"\"\n",
    "\n",
    "        m = Y.shape[0]\n",
    "        Y_expanded = np.zeros_like(A3)\n",
    "        Y_expanded[Y, np.arange(m)] = 1\n",
    "        # vectorized backpropagation\n",
    "        V2 = (A3 - Y_expanded) * A3 * (1 - A3)\n",
    "        V1 = (W2.T @ V2) * A2 * (1 - A2)\n",
    "        \n",
    "        gradW2 = V2 @ A2.T / m\n",
    "        gradW1 = V1 @ A1.T / m\n",
    "        \n",
    "        gradb2 = np.sum(V2, axis=1).reshape((-1,1)) / m\n",
    "        gradb1 = np.sum(V1, axis=1).reshape((-1,1)) / m\n",
    "        \n",
    "        \n",
    "        # regularize weights that are not bias terms\n",
    "        gradW1 += self.l2_C * W1\n",
    "        gradW2 += self.l2_C * W2\n",
    "        return gradW1, gradW2, gradb1, gradb2\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights with Glorot initialization.\"\"\"\n",
    "        limit1 = np.sqrt(6 / (self.n_features_ + self.n_hidden))\n",
    "        self.W1 = np.random.uniform(-limit1, limit1, (self.n_hidden, self.n_features_))\n",
    "        self.b1 = np.zeros((self.n_hidden, 1))\n",
    "        \n",
    "        limit2 = np.sqrt(6 / (self.n_hidden + self.n_output_))\n",
    "        self.W2 = np.random.uniform(-limit2, limit2, (self.n_output_, self.n_hidden))\n",
    "        self.b2 = np.zeros((self.n_output_, 1))\n",
    "        return self.W1, self.W2, self.b1, self.b2\n",
    "    \n",
    "    def _cost(self, A3, y, W1, W2):\n",
    "        \"\"\"Cross-entropy loss without one-hot encoding.\"\"\"\n",
    "        m = y.shape[0]\n",
    "        log_probs = -np.log(A3[y, np.arange(m)])\n",
    "        cost = np.sum(log_probs) / m\n",
    "        L2_term = self._L2_reg(self.l2_C, W1, W2)\n",
    "        return cost + L2_term\n",
    "    \n",
    "    def fit(self, X, y, batch_size=32, print_progress=False):\n",
    "        \"\"\"Training without one-hot encoding\"\"\"\n",
    "        X_data, y_data = X.values, y.values\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.n_features_ = X_data.shape[1]\n",
    "        self.n_output_ = len(np.unique(y_data))  # Set output to number of classes\n",
    "        self.W1, self.W2, self.b1, self.b2 = self._initialize_weights()\n",
    "        \n",
    "        self.cost_ = []\n",
    "        \n",
    "        for i in range(self.epochs):\n",
    "            indices = np.arange(X_data.shape[0])\n",
    "            np.random.shuffle(indices)\n",
    "            \n",
    "            for start_idx in range(0, indices.shape[0] - batch_size + 1, batch_size):\n",
    "                batch_idx = indices[start_idx:start_idx + batch_size]\n",
    "                A1, Z1, A2, Z2, A3 = self._feedforward(X_data[batch_idx], self.W1, self.W2, self.b1, self.b2)\n",
    "                \n",
    "                cost = self._cost(A3, y_data[batch_idx], self.W1, self.W2)\n",
    "                self.cost_.append(cost)\n",
    "                \n",
    "                gradW1, gradW2, gradb1, gradb2 = self._get_gradient(A1=A1, A2=A2, A3=A3, Z1=Z1, Z2=Z2, \n",
    "                                                                    Y=y_data[batch_idx], W1=self.W1, W2=self.W2)\n",
    "                \n",
    "                self.W1 -= self.eta * gradW1\n",
    "                self.W2 -= self.eta * gradW2\n",
    "                self.b1 -= self.eta * gradb1\n",
    "                self.b2 -= self.eta * gradb2\n",
    "            \n",
    "            if print_progress and (i + 1) % print_progress == 0:\n",
    "                sys.stderr.write(f'\\rEpoch: {i + 1}/{self.epochs}')\n",
    "                sys.stderr.flush()\n",
    "                \n",
    "        return self\n",
    "\n",
    "X_train = train_data.drop(columns=['ChildPovertyClass'])\n",
    "y_train = train_data['ChildPovertyClass']\n",
    "X_test = test_data.drop(columns=['ChildPovertyClass'])\n",
    "y_test = test_data['ChildPovertyClass']\n",
    "\n",
    "params = dict(n_hidden=50, \n",
    "              C=0.1, # tradeoff L2 regularizer\n",
    "              epochs=100, # iterations\n",
    "              eta=0.001,  # learning rate\n",
    "              random_state=1)\n",
    "\n",
    "nn = TwoLayerPerceptronVectorized(**params)\n",
    "\n",
    "nn.fit(X_train, y_train, batch_size=32, print_progress=50)\n",
    "yhat = nn.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, yhat)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "\n",
    "plt.plot(nn.cost_)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Convergence')\n",
    "plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
